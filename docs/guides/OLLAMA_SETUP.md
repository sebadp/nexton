# ü¶ô Ollama Setup Guide

Gu√≠a para configurar Ollama para usar con el LinkedIn Agent de forma local y gratuita.

---

## üéØ ¬øPor qu√© Ollama?

- ‚úÖ **Gratis** - Sin API keys ni costos
- ‚úÖ **Local** - Corre en tu m√°quina, privacidad total
- ‚úÖ **R√°pido** - Buena performance con modelos optimizados
- ‚úÖ **F√°cil** - Instalaci√≥n simple

---

## üì• Instalaci√≥n

### macOS / Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS con Homebrew

```bash
brew install ollama
```

### Windows

Descarga desde: https://ollama.com/download/windows

---

## üöÄ Configuraci√≥n

### 1. Iniciar Ollama

```bash
# Inicia el servidor Ollama (corre en background)
ollama serve
```

O simplemente abre la app Ollama si la instalaste con el instalador.

### 2. Descargar Modelos

```bash
# Modelo recomendado: Llama 3.2 (3B) - R√°pido y bueno
ollama pull llama3.2

# O Llama 3.2:1b - M√°s r√°pido, menos preciso
ollama pull llama3.2:1b

# O Llama 3.1:8b - M√°s preciso, m√°s lento
ollama pull llama3.1:8b

# O Mistral - Alternativa muy buena
ollama pull mistral
```

### 3. Probar que funciona

```bash
# Chat interactivo
ollama run llama3.2

# Deber√≠as ver:
>>> Hola, ¬øc√≥mo est√°s?
¬°Hola! Estoy bien, gracias...

# Presiona Ctrl+D para salir
```

### 4. Configurar el proyecto

Edita tu `.env`:

```bash
# LLM Configuration
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
LLM_MAX_TOKENS=500
LLM_TEMPERATURE=0.7

# Ollama URL (default)
OLLAMA_URL=http://localhost:11434
```

---

## üß™ Probar con el proyecto

```bash
python test_message_generation.py
```

Deber√≠as ver:

```
üîß Initializing Components
üß† Initializing DSPy OpportunityAnalyzer...
‚úÖ OpportunityAnalyzer ready
‚úçÔ∏è  Initializing DSPy ResponseGenerator...
‚úÖ ResponseGenerator ready

üì• Step 1: Scraping LinkedIn Messages
...
```

---

## üìä Modelos Recomendados

### Para Testing / Desarrollo

**llama3.2:1b** (1.3 GB)
- M√°s r√°pido
- Bueno para desarrollo
- Respuestas b√°sicas pero funcionales

```bash
ollama pull llama3.2:1b
```

**llama3.2** (2.0 GB) ‚≠ê **Recomendado**
- Balance perfecto velocidad/calidad
- Bueno para producci√≥n
- Respuestas de calidad

```bash
ollama pull llama3.2
```

### Para Producci√≥n

**llama3.1:8b** (4.7 GB)
- Mejor calidad
- M√°s lento
- Respuestas muy buenas

```bash
ollama pull llama3.1:8b
```

**mistral** (4.1 GB)
- Excelente alternativa
- Muy bueno para tareas de an√°lisis
- Respuestas precisas

```bash
ollama pull mistral
```

---

## ‚öôÔ∏è Modelos por M√≥dulo

Puedes usar diferentes modelos para diferentes tareas:

```bash
# En .env
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2

# Analyzer - Puede usar modelo m√°s r√°pido
ANALYZER_LLM_MODEL=llama3.2:1b

# Scorer - Usa modelo default
# (no especificado = usa LLM_MODEL)

# Response Generator - Usa mejor modelo para respuestas
RESPONSE_LLM_MODEL=llama3.1:8b
```

---

## üîß Troubleshooting

### Error: "connection refused" o "Failed to connect to Ollama"

**Causa:** Ollama no est√° corriendo

**Soluci√≥n:**
```bash
# Inicia Ollama
ollama serve

# O verifica que est√© corriendo
curl http://localhost:11434/api/tags
```

### Error: "model not found"

**Causa:** No has descargado el modelo

**Soluci√≥n:**
```bash
# Lista modelos disponibles
ollama list

# Si no est√°, desc√°rgalo
ollama pull llama3.2
```

### Respuestas muy lentas

**Causa:** Modelo demasiado grande para tu hardware

**Soluci√≥n:**
```bash
# Usa modelo m√°s peque√±o
ollama pull llama3.2:1b

# En .env
LLM_MODEL=llama3.2:1b
```

### Respuestas de mala calidad

**Causa:** Modelo demasiado peque√±o

**Soluci√≥n:**
```bash
# Usa modelo m√°s grande
ollama pull llama3.1:8b

# En .env
LLM_MODEL=llama3.1:8b
```

### Error de memoria (Out of Memory)

**Causa:** Tu m√°quina no tiene suficiente RAM

**Soluci√≥n:**
```bash
# Usa el modelo m√°s peque√±o
ollama pull llama3.2:1b

# O cierra otras aplicaciones
```

---

## üöÄ Comandos √ötiles

```bash
# Listar modelos instalados
ollama list

# Ver informaci√≥n de un modelo
ollama show llama3.2

# Eliminar un modelo
ollama rm llama3.2

# Ver logs de Ollama
ollama logs

# Actualizar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Ver modelos disponibles
# Visita: https://ollama.com/library
```

---

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o | RAM Needed | Speed | Quality | Uso Recomendado |
|--------|--------|------------|-------|---------|-----------------|
| llama3.2:1b | 1.3 GB | 4 GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Development/Testing |
| llama3.2 | 2.0 GB | 8 GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | **General Use** |
| llama3.1:8b | 4.7 GB | 16 GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production |
| mistral | 4.1 GB | 16 GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Analysis Tasks |

---

## üîÑ Cambiar de Ollama a Anthropic/OpenAI

Si luego quieres usar API en la nube:

```bash
# En .env, cambia a:
LLM_PROVIDER=anthropic
LLM_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_API_KEY=sk-ant-...

# O OpenAI:
LLM_PROVIDER=openai
LLM_MODEL=gpt-4
OPENAI_API_KEY=sk-...
```

El c√≥digo funciona igual con cualquier provider.

---

## üí° Tips

1. **Primera vez:** Usa `llama3.2` (balance perfecto)
2. **Testing r√°pido:** Usa `llama3.2:1b`
3. **Producci√≥n:** Usa `llama3.1:8b` o `mistral`
4. **An√°lisis complejo:** Usa `llama3.1:8b`
5. **Respuestas simples:** Usa `llama3.2:1b`

---

## üìö Recursos

- Ollama Homepage: https://ollama.com
- Modelos disponibles: https://ollama.com/library
- Ollama GitHub: https://github.com/ollama/ollama
- DSPy + Ollama: https://github.com/stanfordnlp/dspy

---

**¬°Listo!** Ahora puedes usar el LinkedIn Agent sin costos de API üéâ
