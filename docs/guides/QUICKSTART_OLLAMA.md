# ğŸš€ Quick Start con Ollama

GuÃ­a rÃ¡pida para comenzar a usar el LinkedIn Agent con Ollama (gratis y local).

---

## âš¡ Inicio RÃ¡pido (5 minutos)

### 1. Instala Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Descarga el modelo

```bash
ollama pull llama3.2
```

### 3. Inicia Ollama

```bash
ollama serve
```

O simplemente abre la app Ollama si la instalaste con instalador.

### 4. Configura el proyecto

```bash
# Copia el .env de ejemplo
cp .env.example .env

# Edita el .env y configura:
# - LINKEDIN_EMAIL=tu@email.com
# - LINKEDIN_PASSWORD=tupassword
# - LLM_PROVIDER=ollama
# - LLM_MODEL=llama3.2

# O usando comandos:
echo "LINKEDIN_EMAIL=tu@email.com" >> .env
echo "LINKEDIN_PASSWORD='tupassword'" >> .env
echo "LLM_PROVIDER=ollama" >> .env
echo "LLM_MODEL=llama3.2" >> .env
```

### 5. Instala dependencias

```bash
pip install -r requirements.txt
playwright install chromium
```

### 6. Â¡PruÃ©balo!

```bash
python test_message_generation.py
```

---

## ğŸ¯ QuÃ© hace

1. Se conecta a tu LinkedIn
2. Scrapea 3 mensajes recientes
3. Analiza cada mensaje con IA (usando Ollama local)
4. Genera respuesta personalizada automÃ¡tica
5. Muestra todo en la terminal (NO envÃ­a nada)

---

## ğŸ“‹ Output esperado

```
================================================================================
  ğŸ¤– LinkedIn Message Generation Test
================================================================================

ğŸ“§ Email: tu@email.com
ğŸ“ Using DSPy model: ollama/llama3.2

ğŸ”§ Initializing Components
ğŸ§  Initializing DSPy OpportunityAnalyzer...
âœ… OpportunityAnalyzer ready
âœï¸  Initializing DSPy ResponseGenerator...
âœ… ResponseGenerator ready

ğŸ“¥ Step 1: Scraping LinkedIn Messages
ğŸ” Logging in to LinkedIn...
âœ… Login successful!
âœ… Found 3 messages

ğŸ“© Message 1/3
ğŸ‘¤ From: Sarah Johnson - Tech Recruiter
ğŸ’¬ Original Message: Hi! I came across your profile...

ğŸ” Analyzing opportunity...
ğŸ“Š Analysis Results:
   Company: TechCorp
   Role: Senior Backend Engineer
   Salary: $160k-$200k
   Tech Stack: Python, FastAPI, PostgreSQL
   ğŸ“ˆ Total Score: 86/100
   ğŸ¯ Tier: A

âœï¸  Generating AI response...
ğŸ¤– Generated Response:
========================================
Hi Sarah,

Thank you for reaching out! I'm very
interested in learning more about the
Senior Backend Engineer position at
TechCorp...
========================================
```

---

## âš ï¸ Troubleshooting

### "Failed to connect to Ollama"

**SoluciÃ³n:**
```bash
# Verifica que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Si no responde, inÃ­cialo:
ollama serve
```

### "Model not found"

**SoluciÃ³n:**
```bash
# Descarga el modelo
ollama pull llama3.2

# Verifica que estÃ© instalado
ollama list
```

### "Login failed" o "Selector not found"

**SoluciÃ³n:**
```bash
# Ejecuta con browser visible para ver quÃ© pasa
python test_scraper_quick.py

# LinkedIn podrÃ­a necesitar verificaciÃ³n
# Inicia sesiÃ³n manualmente primero en tu navegador
```

### Respuestas muy genÃ©ricas o malas

**SoluciÃ³n:**
```bash
# Usa un modelo mÃ¡s grande
ollama pull llama3.1:8b

# En .env:
LLM_MODEL=llama3.1:8b
```

---

## ğŸ›ï¸ PersonalizaciÃ³n

### Usar modelo mÃ¡s rÃ¡pido

```bash
# En .env
LLM_MODEL=llama3.2:1b
```

### Usar modelo mÃ¡s preciso

```bash
# En .env
LLM_MODEL=llama3.1:8b
```

### Analizar mÃ¡s mensajes

Edita `test_message_generation.py` lÃ­nea 97:

```python
messages = await scraper.scrape_messages(limit=10, unread_only=False)
```

### Solo mensajes no leÃ­dos

```python
messages = await scraper.scrape_messages(limit=5, unread_only=True)
```

---

## ğŸ“š Siguiente paso

Ver documentaciÃ³n completa:
- `OLLAMA_SETUP.md` - Setup detallado de Ollama
- `MESSAGE_GENERATION_TEST.md` - GuÃ­a completa del test script
- `SCRAPER_TESTING_README.md` - Testing del scraper

---

## ğŸ’¡ Comandos Ãºtiles

```bash
# Test solo scraper
python test_scraper_quick.py

# Test con generaciÃ³n de respuestas
python test_message_generation.py

# Ver modelos instalados
ollama list

# Cambiar modelo
ollama pull mistral
# Edita .env: LLM_MODEL=mistral

# Ver logs de Ollama
# En macOS/Linux: Ver terminal donde corre ollama serve
# En Windows: Ver app de Ollama
```

---

**Â¡Listo para empezar!** ğŸ‰

Si algo no funciona, revisa las guÃ­as detalladas o los logs de error.
